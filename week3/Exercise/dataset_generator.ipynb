{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Generador de Datasets con Modelos de Lenguaje\n",
    "\n",
    "Este notebook te permite generar datasets utilizando diversos modelos:\n",
    "- **Modelos de Hugging Face** (gratuitos, ejecutados localmente o via Inference API)\n",
    "- **Modelos Frontera** (GPT-4, Claude, Gemini via APIs)\n",
    "\n",
    "## üìã √çndice\n",
    "1. Instalaci√≥n de dependencias\n",
    "2. Configuraci√≥n de APIs\n",
    "3. Modelos de Hugging Face\n",
    "4. Modelos Frontera (APIs)\n",
    "5. Generaci√≥n de Datasets\n",
    "6. Exportaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q openai anthropic google-generativeai\n",
    "!pip install -q datasets pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de APIs\n",
    "\n",
    "Configura tus claves API aqu√≠. Puedes usar Google Colab Secrets para mayor seguridad:\n",
    "- Ve a (icono de llave) en el panel izquierdo\n",
    "- A√±ade tus secrets con los nombres correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opci√≥n 1: Usando Google Colab Secrets (recomendado)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    # Opci√≥n 2: Configuraci√≥n directa (menos seguro)\n",
    "    OPENAI_API_KEY = \"\"  # tu-api-key-aqui\n",
    "    ANTHROPIC_API_KEY = \"\"  # tu-api-key-aqui\n",
    "    GOOGLE_API_KEY = \"\"  # tu-api-key-aqui\n",
    "    HF_TOKEN = \"\"  # tu-huggingface-token-aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n de Modelos de Hugging Face\n",
    "\n",
    "Aqu√≠ configuramos modelos open-source que se pueden ejecutar en Colab (con limitaciones de GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceGenerator:\n",
    "    \"\"\"Generador usando modelos de Hugging Face\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\", use_4bit: bool = True):\n",
    "        \"\"\"\n",
    "        Modelos recomendados para Colab (gratuito):\n",
    "        - meta-llama/Llama-3.2-3B-Instruct (3B par√°metros)\n",
    "        - mistralai/Mistral-7B-Instruct-v0.3 (7B par√°metros, requiere m√°s GPU)\n",
    "        - google/gemma-2-2b-it (2B par√°metros, muy ligero)\n",
    "        - Qwen/Qwen2.5-3B-Instruct (3B par√°metros)\n",
    "        \"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "        import torch\n",
    "\n",
    "        self.model_name = model_name\n",
    "        print(f\"Cargando modelo: {model_name}\")\n",
    "\n",
    "        # Configuraci√≥n para reducir uso de memoria\n",
    "        if use_4bit:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            quantization_config = None\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        print(\"Modelo cargado exitosamente\")\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Genera texto a partir de un prompt\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuraci√≥n de Modelos Frontera (APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGenerator:\n",
    "    \"\"\"Generador usando OpenAI (GPT-4, GPT-4o, GPT-3.5)\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Modelos disponibles:\n",
    "        - gpt-4o: El m√°s potente y reciente\n",
    "        - gpt-4o-mini: R√°pido y econ√≥mico\n",
    "        - gpt-4-turbo: Versi√≥n anterior de GPT-4\n",
    "        - gpt-3.5-turbo: M√°s econ√≥mico\n",
    "        \"\"\"\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(api_key=api_key or OPENAI_API_KEY)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "class AnthropicGenerator:\n",
    "    \"\"\"Generador usando Claude (Anthropic)\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"claude-3-5-sonnet-20241022\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Modelos disponibles:\n",
    "        - claude-3-5-sonnet-20241022: El m√°s reciente y potente\n",
    "        - claude-3-5-haiku-20241022: R√°pido y econ√≥mico\n",
    "        - claude-3-opus-20240229: M√°xima calidad (m√°s caro)\n",
    "        \"\"\"\n",
    "        from anthropic import Anthropic\n",
    "        self.client = Anthropic(api_key=api_key or ANTHROPIC_API_KEY)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "\n",
    "class GeminiGenerator:\n",
    "    \"\"\"Generador usando Gemini (Google)\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"gemini-2.0-flash-exp\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Modelos disponibles:\n",
    "        - gemini-2.0-flash-exp: Experimental, muy r√°pido\n",
    "        - gemini-1.5-pro: Potente y con contexto largo\n",
    "        - gemini-1.5-flash: R√°pido y econ√≥mico\n",
    "        \"\"\"\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=api_key or GOOGLE_API_KEY)\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n",
    "        response = self.model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                'temperature': temperature,\n",
    "                'max_output_tokens': max_tokens,\n",
    "            }\n",
    "        )\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clase Principal para Generaci√≥n de Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"Clase principal para generar datasets con m√∫ltiples modelos\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.generators = {}\n",
    "        self.results = []\n",
    "\n",
    "    def add_generator(self, name: str, generator):\n",
    "        \"\"\"A√±ade un generador a la lista\"\"\"\n",
    "        self.generators[name] = generator\n",
    "        print(f\"Generador '{name}' a√±adido\")\n",
    "\n",
    "    def generate_dataset(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        generator_names: Optional[List[str]] = None,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        delay: float = 1.0\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Genera un dataset usando los prompts y generadores especificados\n",
    "\n",
    "        Args:\n",
    "            prompts: Lista de prompts para generar\n",
    "            generator_names: Nombres de los generadores a usar (None = todos)\n",
    "            max_tokens: M√°ximo de tokens por generaci√≥n\n",
    "            temperature: Temperatura para la generaci√≥n (0-2)\n",
    "            delay: Tiempo de espera entre llamadas (para evitar rate limits)\n",
    "        \"\"\"\n",
    "        if generator_names is None:\n",
    "            generator_names = list(self.generators.keys())\n",
    "\n",
    "        results = []\n",
    "\n",
    "        total_iterations = len(prompts) * len(generator_names)\n",
    "        pbar = tqdm(total=total_iterations, desc=\"Generando dataset\")\n",
    "\n",
    "        for prompt_idx, prompt in enumerate(prompts):\n",
    "            for gen_name in generator_names:\n",
    "                if gen_name not in self.generators:\n",
    "                    print(f\"‚ö†Ô∏è Generador '{gen_name}' no encontrado, saltando...\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    generator = self.generators[gen_name]\n",
    "                    response = generator.generate(\n",
    "                        prompt=prompt,\n",
    "                        max_tokens=max_tokens,\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "\n",
    "                    results.append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt': prompt,\n",
    "                        'model': gen_name,\n",
    "                        'response': response,\n",
    "                        'temperature': temperature,\n",
    "                        'max_tokens': max_tokens\n",
    "                    })\n",
    "\n",
    "                    pbar.set_postfix({'√∫ltimo_modelo': gen_name})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n‚ùå Error con {gen_name}: {str(e)}\")\n",
    "                    results.append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt': prompt,\n",
    "                        'model': gen_name,\n",
    "                        'response': f\"ERROR: {str(e)}\",\n",
    "                        'temperature': temperature,\n",
    "                        'max_tokens': max_tokens\n",
    "                    })\n",
    "\n",
    "                pbar.update(1)\n",
    "                time.sleep(delay)  # Evitar rate limits\n",
    "\n",
    "        pbar.close()\n",
    "        self.results.extend(results)\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"\\n‚úÖ Dataset generado: {len(df)} filas\")\n",
    "        return df\n",
    "\n",
    "    def save_results(self, filename: str, format: str = 'csv'):\n",
    "        \"\"\"Guarda los resultados en el formato especificado\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "\n",
    "        if format == 'csv':\n",
    "            df.to_csv(filename, index=False)\n",
    "        elif format == 'json':\n",
    "            df.to_json(filename, orient='records', indent=2, force_ascii=False)\n",
    "        elif format == 'jsonl':\n",
    "            df.to_json(filename, orient='records', lines=True, force_ascii=False)\n",
    "        elif format == 'parquet':\n",
    "            df.to_parquet(filename, index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Formato no soportado: {format}\")\n",
    "\n",
    "        print(f\"üíæ Resultados guardados en: {filename}\")\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejemplos de Uso\n",
    "\n",
    "### Ejemplo 1: Usar solo modelos de API (sin GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar generador de datasets\n",
    "dataset_gen = DatasetGenerator()\n",
    "\n",
    "# A√±adir generadores de APIs (no requieren GPU)\n",
    "if OPENAI_API_KEY:\n",
    "    dataset_gen.add_generator(\"gpt-4o-mini\", OpenAIGenerator(\"gpt-4o-mini\"))\n",
    "\n",
    "if ANTHROPIC_API_KEY:\n",
    "    dataset_gen.add_generator(\"claude-sonnet\", AnthropicGenerator(\"claude-3-5-sonnet-20241022\"))\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    dataset_gen.add_generator(\"gemini-flash\", GeminiGenerator(\"gemini-2.0-flash-exp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: Usar modelos de Hugging Face (requiere GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda requiere GPU. Aseg√∫rate de tener GPU habilitada:\n",
    "# Runtime > Change runtime type > T4 GPU\n",
    "\n",
    "# Cargar un modelo peque√±o de Hugging Face\n",
    "try:\n",
    "    hf_gen = HuggingFaceGenerator(\n",
    "        model_name=\"google/gemma-2-2b-it\",  # Modelo peque√±o, funciona en Colab gratuito\n",
    "        use_4bit=True  # Cuantizaci√≥n para ahorrar memoria\n",
    "    )\n",
    "    dataset_gen.add_generator(\"gemma-2b\", hf_gen)\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar el modelo HF: {e}\")\n",
    "    print(\"Continuando solo con APIs...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir Prompts para tu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Dataset de preguntas-respuestas en espa√±ol\n",
    "prompts_qa = [\n",
    "    \"¬øCu√°l es la capital de Espa√±a y cu√°ntos habitantes tiene?\",\n",
    "    \"Explica qu√© es la inteligencia artificial en t√©rminos simples.\",\n",
    "    \"¬øCu√°les son los beneficios de hacer ejercicio regularmente?\",\n",
    "    \"Describe el proceso de fotos√≠ntesis en las plantas.\",\n",
    "    \"¬øQu√© es el cambio clim√°tico y cu√°les son sus principales causas?\"\n",
    "]\n",
    "\n",
    "# Ejemplo 2: Dataset para generaci√≥n de c√≥digo\n",
    "prompts_code = [\n",
    "    \"Escribe una funci√≥n en Python que calcule el factorial de un n√∫mero.\",\n",
    "    \"Crea una funci√≥n JavaScript que valide si un email es v√°lido.\",\n",
    "    \"Genera c√≥digo SQL para crear una tabla de usuarios con id, nombre, email y fecha_registro.\"\n",
    "]\n",
    "\n",
    "# Ejemplo 3: Dataset de creative writing\n",
    "prompts_creative = [\n",
    "    \"Escribe un p√°rrafo describiendo un atardecer en la playa.\",\n",
    "    \"Crea el inicio de un cuento de ciencia ficci√≥n sobre robots.\",\n",
    "    \"Escribe un poema corto sobre la amistad.\"\n",
    "]\n",
    "\n",
    "# Selecciona qu√© prompts usar\n",
    "prompts_to_use = prompts_qa  # Cambia esto seg√∫n necesites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el dataset\n",
    "df = dataset_gen.generate_dataset(\n",
    "    prompts=prompts_to_use,\n",
    "    max_tokens=512,\n",
    "    temperature=0.7,\n",
    "    delay=1.0  # Espera 1 segundo entre llamadas\n",
    ")\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas del dataset\n",
    "print(f\"Total de generaciones: {len(df)}\")\n",
    "print(f\"Total de prompts √∫nicos: {df['prompt_id'].nunique()}\")\n",
    "print(f\"Modelos usados: {df['model'].unique().tolist()}\")\n",
    "print(\"\\nDistribuci√≥n por modelo:\")\n",
    "print(df['model'].value_counts())\n",
    "\n",
    "# Longitud promedio de las respuestas\n",
    "df['response_length'] = df['response'].str.len()\n",
    "print(\"\\nLongitud promedio de respuestas por modelo:\")\n",
    "print(df.groupby('model')['response_length'].mean().round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en diferentes formatos\n",
    "dataset_gen.save_results('dataset_generated.csv', format='csv')\n",
    "dataset_gen.save_results('dataset_generated.json', format='json')\n",
    "dataset_gen.save_results('dataset_generated.jsonl', format='jsonl')\n",
    "\n",
    "# Descargar archivos (en Colab)\n",
    "from google.colab import files\n",
    "files.download('dataset_generated.csv')\n",
    "files.download('dataset_generated.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo Avanzado: Generaci√≥n con Variaciones de Temperatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar con diferentes temperaturas para comparar creatividad\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "prompt_test = \"Escribe un p√°rrafo sobre el futuro de la IA.\"\n",
    "\n",
    "results_temp = []\n",
    "\n",
    "for temp in temperatures:\n",
    "    for gen_name, generator in dataset_gen.generators.items():\n",
    "        try:\n",
    "            response = generator.generate(\n",
    "                prompt=prompt_test,\n",
    "                max_tokens=200,\n",
    "                temperature=temp\n",
    "            )\n",
    "            results_temp.append({\n",
    "                'model': gen_name,\n",
    "                'temperature': temp,\n",
    "                'response': response\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "df_temp = pd.DataFrame(results_temp)\n",
    "display(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Recursos Adicionales\n",
    "\n",
    "### Modelos de Hugging Face Recomendados (por tama√±o):\n",
    "\n",
    "**Peque√±os (2-3B) - Funcionan en Colab gratuito:**\n",
    "- `google/gemma-2-2b-it`\n",
    "- `meta-llama/Llama-3.2-3B-Instruct`\n",
    "- `Qwen/Qwen2.5-3B-Instruct`\n",
    "\n",
    "**Medianos (7-8B) - Requieren Colab Pro:**\n",
    "- `meta-llama/Llama-3.1-8B-Instruct`\n",
    "- `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "\n",
    "### APIs de Modelos Frontera:\n",
    "\n",
    "**OpenAI:** https://platform.openai.com/\n",
    "- GPT-4o, GPT-4o-mini, GPT-3.5-turbo\n",
    "\n",
    "**Anthropic:** https://console.anthropic.com/\n",
    "- Claude 3.5 Sonnet, Claude 3.5 Haiku\n",
    "\n",
    "**Google:** https://ai.google.dev/\n",
    "- Gemini 2.0 Flash, Gemini 1.5 Pro\n",
    "\n",
    "### Tips:\n",
    "- Usa **temperatura baja (0.1-0.4)** para tareas que requieren precisi√≥n\n",
    "- Usa **temperatura alta (0.7-1.2)** para tareas creativas\n",
    "- Para datasets grandes, considera usar batch APIs cuando est√©n disponibles\n",
    "- Guarda frecuentemente tus resultados para evitar p√©rdida de datos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
